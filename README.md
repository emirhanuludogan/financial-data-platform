# Financial Data Platform (Spark ETL Pipeline)

Bu proje, TCMB (EVDS) Ã¼zerinden alÄ±nan finansal verilerin **PySpark** kullanÄ±larak iÅŸlendiÄŸi, temizlendiÄŸi ve analize hazÄ±r hale getirildiÄŸi uÃ§tan uca bir **Veri MÃ¼hendisliÄŸi (ETL)** pipeline Ã§alÄ±ÅŸmasÄ±dÄ±r.
Proje, kurumsal standartlara uygun olarak Medallion Architecture (Bronze, Silver, Gold) prensipleriyle yapÄ±landÄ±rÄ±lmÄ±ÅŸtÄ±r.

---

## Ã–ne Ã‡Ä±kan Ã–zellikler

* **Dinamik Veri AlÄ±mÄ±:** TCMB EVDS API entegrasyonu ile gerÃ§ek zamanlÄ±ya yakÄ±n veri Ã§ekimi.
* **BÃ¼yÃ¼k Veri Ä°ÅŸleme:** PySpark kullanarak verinin daÄŸÄ±tÄ±k mimaride iÅŸlenmesi.
* **Feature Engineering:** `Window Functions` ve `Lag` metotlarÄ± kullanÄ±larak gÃ¼nlÃ¼k dÃ¶viz kuru deÄŸiÅŸim yÃ¼zdelerinin hesaplanmasÄ±.
* **GÃ¶zlemlenebilirlik (Observability):** Merkezi logging sistemi ile tÃ¼m pipeline adÄ±mlarÄ±nÄ±n ve Ã§alÄ±ÅŸma sÃ¼relerinin (**runtime**) anlÄ±k takibi.
* **Veri Kalitesi (Data Quality):** Eksik verilerin (Null/NaN) temizlenmesi ve ÅŸema (schema) doÄŸrulama sÃ¼reÃ§leri.
* **KurÅŸun GeÃ§irmez Dosya YÃ¶netimi:** Windows iÅŸletim sistemindeki dosya kilitlenme sorunlarÄ±nÄ± aÅŸmak iÃ§in shutil kÃ¼tÃ¼phanesi ile hibrit temizlik stratejisi.
* **Cross-Platform Path YÃ¶netimi:** Pathlib kullanÄ±larak Windows/Linux baÄŸÄ±msÄ±z dinamik kÃ¶k dizin (root) tespiti.
* **Ã‡oklu Depolama:** Verilerin hem insan-okunabilir (**CSV**) hem de performans odaklÄ± (**Parquet**) formatlarda kaydedilmesi.
* **ModÃ¼ler:** Script YapÄ±sÄ±: Her ETL adÄ±mÄ±nÄ±n (**Ingestion, Processing, Features**) ayrÄ± ve tek sorumluluÄŸa sahip scriptler tarafÄ±ndan yÃ¶netilmesi.
* **Orkestrasyon:** TÃ¼m sÃ¼recin merkezi bir main.py Ã¼zerinden yÃ¶netilmesi.
* **Otomasyon DesteÄŸi:** Pipeline'Ä±n her sabah otomatik Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlayan .bat orkestrasyon desteÄŸi.

---
##  Veri Mimarisi (Medallion Architecture)

Proje akÄ±ÅŸÄ±, veriyi ham halinden analitik deÄŸere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in ÅŸu mantÄ±ksal hattÄ± takip eder:

 Raw (Bronze): EVDS API Ã¼zerinden Ã§ekilen ham verilerin hiÃ§bir deÄŸiÅŸiklik yapÄ±lmadan Parquet formatÄ±nda saklandÄ±ÄŸÄ±, "deÄŸiÅŸmez" (**immutable**) katman.

Silver (Processed): Veri tiplerinin dÃ¼zenlendiÄŸi, eksik verilerin temizlendiÄŸi ve verinin analize uygun hale getirildiÄŸi "temiz" katman.

Gold (Analytics): Ä°ÅŸ mantÄ±ÄŸÄ± (**Business Logic**) eklenerek finansal Ã¶zelliklerin hesaplandÄ±ÄŸÄ±, nihai raporlama katmanÄ±.

---

##  Teknoloji Stack'i

* **Dil:** Python 3.10.11
* **Framework:** Apache Spark 
* **KÃ¼tÃ¼phaneler:**  Python-dotenv, evds, Pathlib, Shutil, Logging
* **Veri KaynaÄŸÄ±:** TCMB EVDS API
* **Depolama:** Parquet, CSV

---

##  Proje YapÄ±sÄ±

â”œâ”€â”€ data/               # Git-ignored: Yerel veri depolama (Raw, Silver, Gold)
â”‚   â”œâ”€â”€ raw/            # Ham verilerin saklandÄ±ÄŸÄ± katman (Bronze)
â”‚   â”œâ”€â”€ silver/         # TemizlenmiÅŸ verilerin saklandÄ±ÄŸÄ± katman
â”‚   â””â”€â”€ gold/           # Analiz ve rapor hazÄ±r verilerin saklandÄ±ÄŸÄ± katman
â”œâ”€â”€ logs/               # Git-ignored: Zaman mÃ¼hÃ¼rlÃ¼ pipeline gÃ¼nlÃ¼kleri
â”‚   â””â”€â”€ automation.log  # Merkezi log dosyasÄ± (TÃ¼m ETL adÄ±mlarÄ± burada tutulur)
â”œâ”€â”€ notebooks/          # Veri keÅŸfi ve demo gÃ¶rselleÅŸtirmeler
â”œâ”€â”€ src/                # ETL Pipeline modÃ¼lleri
â”‚   â”œâ”€â”€ ingestion/      # Veri alÄ±mÄ± (Raw Layer)
â”‚   â”‚   â””â”€â”€ ingestion.py
â”‚   â”œâ”€â”€ processing.py   # Veri temizleme (Silver Layer)
â”‚   â”œâ”€â”€ features.py     # Ã–zellik mÃ¼hendisliÄŸi (Gold Layer)
â”‚   â””â”€â”€ utils.py        # Merkezi logger ve Spark konfigÃ¼rasyonu
â”œâ”€â”€ .env                # Git-ignored: API Key yÃ¶netimi
â”œâ”€â”€ .gitignore          # Gereksiz dosyalarÄ±n takibini engelleyen liste
â”œâ”€â”€ main.py             # Pipeline Orkestrasyon (Åef) Scripti
â”œâ”€â”€  run_pipeline.bat   # Windows Otomasyon Tetikleyicisi (Tek tÄ±kla tÃ¼m akÄ±ÅŸÄ± baÅŸlatÄ±r)
â””â”€â”€ requirements.txt    #  Proje baÄŸÄ±mlÄ±lÄ±klarÄ± (pyspark, pathlib, shutil, python-dotenv eklendi)
------------------------------------------------------------------------------------------------------
 Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

Projeyi yerel ortamÄ±nÄ±zda Ã§alÄ±ÅŸtÄ±rmak iÃ§in aÅŸaÄŸÄ±daki adÄ±mlarÄ± sÄ±rasÄ±yla takip edin.

1ï¸âƒ£ Depoyu KlonlayÄ±n

Terminalinizi aÃ§Ä±n ve projeyi bilgisayarÄ±nÄ±za indirin:

git clone https://github.com/emirhanuludogan/financial-data-platform.git
cd financial-data-platform

------------------------------------------------------------------------------------------------------

2ï¸âƒ£ Gerekli KÃ¼tÃ¼phaneleri YÃ¼kleyin
Bash
pip install -r requirements.txt

âš ï¸ Python 3.10+ kullanmanÄ±z Ã¶nerilir.
 
 ------------------------------------------------------------------------------------------------------ 
  
3ï¸âƒ£ API YapÄ±landÄ±rmasÄ± 

Projenin kÃ¶k dizininde .env adlÄ± bir dosya oluÅŸturun ve
TCMB (EVDS) Ã¼zerinden aldÄ±ÄŸÄ±nÄ±z API anahtarÄ±nÄ± aÅŸaÄŸÄ±daki formatta ekleyin:

EVDS_API_KEY=buraya_api_anahtarinizi_yazin

ğŸ” .env dosyasÄ± gÃ¼venlik sebebiyle .gitignore iÃ§inde yer almaktadÄ±r.

------------------------------------------------------------------------------------------------------
4ï¸âƒ£ Pipeline'Ä± BaÅŸlatÄ±n

Bash
TÃ¼m sÃ¼reci merkezi orkestratÃ¶r Ã¼zerinden tetikleyin:
Bash
python main.py

Alternatif olarak analiz sÃ¼recini gÃ¶zlemlemek iÃ§in notebooks/demo.ipynb dosyasÄ±nÄ± kullanabilirsiniz.

Alternatif (Windows Otomasyon): Pipeline'Ä± her sabah otomatik Ã§alÄ±ÅŸtÄ±rmak iÃ§in run_pipeline.bat dosyasÄ±nÄ± Windows GÃ¶rev ZamanlayÄ±cÄ±'ya tanÄ±mlayabilirsiniz.

------------------------------------------------------------------------------------------------------

 ## Windows Ä°Ã§in MÃ¼hendislik Ã‡Ã¶zÃ¼mleri

Windows kÄ±sÄ±tlamalarÄ± sebebiyle Spark'Ä±n yaÅŸadÄ±ÄŸÄ± Unable to clear output directory hatasÄ± ÅŸu yÃ¶ntemlerle Ã§Ã¶zÃ¼lmÃ¼ÅŸtÃ¼r:

Hybrid Write: Yazma iÅŸlemi Ã¶ncesi shutil.rmtree(path, ignore_errors=True) ile dosya sistemine doÄŸrudan mÃ¼dahale.

Single Partition (Coalesce): .coalesce(1) kullanÄ±larak yÃ¼zlerce kÃ¼Ã§Ã¼k .crc dosyasÄ±nÄ±n kilitlenmesi engellenmiÅŸ ve raporlama performansÄ± artÄ±rÄ±lmÄ±ÅŸtÄ±r.
------------------------------------------------------------------------------------------------------

## KaynakÃ§a (References)
Bu projenin mimarisi ve ETL sÃ¼reÃ§leri aÅŸaÄŸÄ±daki modern metodoloji takip edilerek geliÅŸtirilmiÅŸtir:

* **Matt Palmer** - *Understanding ETL: Data Pipelines for Modern Data Architectures* (2024, O'Reilly Media, Inc.)

## ğŸ¤ TeÅŸekkÃ¼r (Acknowledgments)
* Teknik istiÅŸareleri ve desteÄŸi iÃ§in **[Onur GÃ¼ner]**'e teÅŸekkÃ¼rler.

------------------------------------------------------------------------------------------------------
